## L8 Lesson Plan: Explainable vs Non-Explainable AI
### Lesson Description:
In this lesson, students will explore the fundamental differences between explainable AI (XAI) and non-explainable AI (Black Box AI), identifying real-world examples and analyzing their strengths and weaknesses. Through hands-on activities, students will see examples of how each AI functions, helping them understand the importance of transparency, trust, accuracy, and regulatory compliance in AI systems. By the end of the lesson, students will evaluate the trade-offs of each approach and justify when one might be preferable over the other, particularly in fields like agriculture.  

### Main Learning Goal:
Students will be able to explain the key differences between explainable and non-explainable AI, assess their advantages and limitations, and determine their appropriate applications based on factors such as accuracy, transparency, and ethical considerations.  

### Essential Question:
What makes AI explainable or non-explainable, and how do these two approaches compare in terms of decision-making, trust, and real-world applications?  

### Standards:
•	IAI.A2.1 Articulate the impact that computing devices and AI have in real-world settings (e.g., traffic lights, medical devices, facial recognition).

### Objectives:
•	Students will be able to define and differentiate explainable and non-explainable AI

•	Students will be able to identify examples of both types of AI

•	Students will explore and analyze how decision trees and neural networks function

•	Students will evaluate the importance of transparency, trust, and regulatory compliance in AI decision-making

•	Students will be able to justify the selection of explainable AI, non-explainable AI, or a hybrid approach based on specific use cases


### Total Duration: 60 minutes (1 days)

#### Important Vocabulary:

1.	Explainable AI (XAI)

    a.	Definition: A set of processes and methods that allow human users to comprehend and trust the results and output created by machine learning algorithms.

    b.	Example: Decision trees, linear regression, and logistic regression

2.	Non-Explainable AI (Black Box AI)

    a.	Definition: An AI system whose internal workings are a mystery to its users.

    b.	Example: Deep learning, neural networks, and random forests

3.	Deep Learning

    a.	Definition: A subset of machine learning that uses multilayered neural networks, called deep neural networks, to simulate the complex decision-making power of the human brain.

    b.	Example: Facial recognition systems

4.	Decision Tree

    a.	Definition: A flowchart-like model that makes decisions by branching into multiple paths based on conditions or features. Each node represents a decision, and each branch leads to an outcome. 

    b.	Example: Medical diagnosis

5.	Neural Network

    a.	Definition: A neural network is a machine learning program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions.

    b.	Example: Chatbots like ChatGPT

6.	Transparency

    a.	Definition: Helps people access information to better understand how an artificial intelligence (AI) system was created and how it makes decisions.

    b.	Example: Credit scoring AI

7.	Regulatory Compliance

    a.	Definition: Organization's adherence to laws, regulations, and standards established by government agencies or industry bodies

    b.	Example: The EU’s General Data Protection Regulation (GDPR) requires companies to provide explanations for AI-driven decisions

8.	Bias

    a.	Definition: The occurrence of biased results due to human biases that skew the original training data or AI algorithm—leading to distorted outputs and potentially harmful outcomes.

    b.	Example: Training a model on a small subpopulation produces a biased model


### Engage (Elicit/Develop) ~ 10 minutes

#### Activity #1: Now You See Me, Now You Don’t (~10 minutes)

**Activity Walkthrough:** 

1.	The teacher will begin the lesson with a quick demonstration.

    a.	Part 1: The teacher will announce that they are writing numbers 1 – 5 on the board and tell students to close their eyes and put their heads down. Students will be asked to remember the exact order the numbers were written.

    b.	Part 2: The teacher will rewrite the numbers with students watching this time. Students must try to recall the order the numbers were written (it should be easier this time, since they were able to see).

2.	The teacher will connect the activity to AI, by introducing explainable AI (XAI) and non-explainable AI (black box AI).

    a.	In part 1, students could not see and had no insight into the process, similar to non-explainable AI, where users can only see the input and output, but have no insight into how it makes a decision.

    b.	In part 2, students could see the process, similar to explainable AI, where there is transparency in the decision process that helps users understand the decision-making process.

3.	The teacher will prompt students to think about real-world AI applications, and to consider the following questions:

    a.	Q1: Imagine being diagnosed in a hospital but not knowing how the AI reached its conclusion. Would you trust that diagnosis? 

    Students should consider the potential risks of accepting an AI-generated diagnosis based on their knowledge (or lack thereof) of the system being used or the type of information that is being used by the system. They may also consider their own personal feelings, such as how much they trust AI in critical situations such as healthcare, and if they would want an explanation or human confirmation.

    b.	Q2: What if a bank denied your loan, but couldn’t explain why?

    Students should consider how a lack of transparency affects their ability to understand the decision, or the fairness of an AI decision without human oversight.

4.	The teacher will show students a video about Explainable and Non-Explainable AI (video is 6:52 min long, stop at 5:32).

5.	Students should follow along using the **SREB_U3_L8_Handout_AIinaBox**, considering and writing down their answer to the following question:

    a.	Q1: Why do you think transparency and trust are important?

    b.	Q2: Write down three things you learned from the “Explainable AI explained!” video.

    On top of writing down what they have learned, students should fill out the Pros and Cons table found in the handout below Q2.


### Explore Pt. 1 (Develop) ~ 30 minutes

#### Activity #1: Explainable AI (XAI) (~15 minutes)

**Activity Walkthrough:** 

1.	The teacher will introduce, define, and lecture on Explainable AI, why it is useful, as well as its benefits and limitations.

2.	The teacher will introduce decision trees (a type of XAI model) and show students a YouTube video (Visual Guide to Decision Trees - 6:25 min) explaining how they work.

3.	Students will complete a practice activity, where they are provided with a sample data set worksheet with tree heights and diameter measurements. They must classify and record different tree species based on their characteristics and discuss their answers as a class. 

4.	Students should make observations and try to understand how structured data allows for transparency and more interpretable data.

#### Activity #2: Non-Explainable AI (Black Box AI) (~15 minutes)

**Activity Walkthrough:** 

1. The teacher will define and lecture on Non-Explainable AI, how it is used and why it can be beneficial, and the concerns/issues associated with its lack of transparency.

2.	The teacher will introduce neural networks (a type of non-explainable AI) and show a video to provide students with a brief overview of how they work.

    a.	After watching the video, students should fill out the pros and cons table for Explainable and Non-Explainable AI found in the SREB_U3_L8_Handout_AIinaBox, using what they have learned about both.

3.	The teacher will guide students through a neural network demo activity (A Neural Network Playground), walking them through the steps to understand how AI processes inputs to generate outputs.

4.	Students should be able to observe, understand and discuss why decision-making, and the systems developed around it are powerful yet complex, and ultimately why transparency is important and where complexity is needed for real-world AI applications.


### Explore Pt. 2 (Deploy) ~ 15 minutes 
#### Activity Name: The AI Farmer’s Dilemma (~15 minutes)

**Activity Walkthrough:** 

1.	The teacher will guide students through the main activity of the lesson. In groups of 3 – 4, students will act as farmers trying to maximize their crop yield and profitability, having to consider various factors (soil conditions, weather patterns, crop history). For each factor, they will be given two articles where they will analyze and compare the two AI approaches (Explainable and Non-Explainable) for each factor and decide which one is the best choice for each factor.

    a.	Students should record their notes for their decision-making for this activity using the table in the SREB_U3_L8_Handout_AIinaBox.

    b.	Teachers should help students throughout the process, helping point out things to look for if they get lost or confused and making sure each group is on the right track, as well as answering questions anyone may have.

    c.	Students should call back to what they’ve learned about the complexities of decision-making, and transparency, trustworthiness, and interpretability of these systems. 

2.	The teacher (if time permits) will facilitate group presentations, where each group will explain their choices and their decision-making processes and tradeoffs for each factor.

### Explain (Refine) ~ 5 minutes
#### Activity Name: Exit Pass (~5 minutes)

**Activity Walkthrough:** 

1.	The students will complete an exit pass before they leave, answering the following question:

    a.	**Q1: How does the choice between explainable and non-explainable AI impact trust and decision-making in real-world applications?**  

    Students should consider how explainable AI builds trusts through providing reasoning and transparency (behind a process or system) behind decisions. They should also think about the trade-off between accuracy and transparency in different AI applications, as well as situations where non-explainable AI may still be useful.
